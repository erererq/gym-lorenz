import gymnasium as gym
from gymnasium import spaces
import numpy as np
from stable_baselines3 import PPO
import torch

def hr_derivatives(state, a1, a2, a, b, c, d, r, s, I_bias, x_rest):
    x1, x2, x3 = state
    dx1 = x2 - a*(x1**3) + b*(x1**2) - x3 + I_bias
    dx2 = c - d*(x1**2) - x2 + a1
    dx3 = r * (s * (x1 - x_rest) - x3) + a2
    return np.array([dx1, dx2, dx3])
class HRSyncEnv(gym.Env):
    """
    Hindmarsh-Rose ç¥ç»å…ƒåŒæ­¥ç¯å¢ƒ
    Master ç³»ç»Ÿ: è‡ªç”±è¿è¡Œ
    Slave ç³»ç»Ÿ: å— RL Agent æ§åˆ¶
    """
    def __init__(self, add_noise=False, eval_mode=False,add_filter=False):
        super().__init__()
        self.add_noise = add_noise  # å¼€å…³ï¼šæ˜¯å¦æ·»åŠ å™ªå£°
        self.eval_mode = eval_mode  # å¼€å…³ï¼šæ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼
        self.add_filter = add_filter  # å¼€å…³ï¼šæ˜¯å¦å¯ç”¨åŠ¨ä½œæ»¤æ³¢å™¨
        
        # 1. å®šä¹‰åŠ¨ä½œç©ºé—´: è¿ç»­å€¼ï¼Œä»£è¡¨æ§åˆ¶ç”µæµ u
        # å‡è®¾æ§åˆ¶ç”µæµèŒƒå›´åœ¨ [-1.0, 1.0] ä¹‹é—´
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)
        
        # 2. å®šä¹‰çŠ¶æ€ç©ºé—´: [x_m, y_m, z_m, x_s, y_s, z_s]
        # æˆ–è€…ç®€åŒ–ä¸ºè¯¯å·®ç³»ç»Ÿ [ex, ey, ez]ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨è¯¯å·®ï¼Œè®© Agent è‡ªå·±å­¦ä¹ ç‰¹å¾ï¼Œè¿˜æœ‰ç»å¯¹ä½ç½®ä¸‰ä¸ªç»´åº¦
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(6,), dtype=np.float32)
        self.scale_factor = 50.0  # å®šä¹‰ç¼©æ”¾å› å­
        # HR æ¨¡å‹å‚æ•° (æ ¹æ®è®ºæ–‡è°ƒæ•´)
        self.a, self.b, self.c, self.d = 1.0, 3.0, 1.0, 5.0
        self.r, self.s, self.I_bias, self.x_rest = 0.006, 4.0, 3.2, -1.6
        self.dt = 0.001  # ä»¿çœŸæ­¥é•¿
        self.sigma=0.0 # å™ªå£°å¼ºåº¦ï¼Œé»˜è®¤ä¸æ·»åŠ å™ªå£°
        
        # ğŸŒŸ ä»…æ–°å¢ï¼šåŠ¨ä½œæ»¤æ³¢å™¨çš„è®°å¿†å˜é‡å’Œç³»æ•°
        self.action_alpha = 0.95  # åŠ¨ä½œå¹³æ»‘ç³»æ•° (è°ƒå‚é‡ç‚¹ï¼ä¸èƒ½å¤ªå°ï¼Œå¦åˆ™åŠ¨ä½œæœ‰å»¶è¿Ÿä¼šå¯¼è‡´æ— æ³•åŒæ­¥)
        self.filtered_action = np.zeros(2, dtype=np.float32) # è®°å¿†ä¸Šä¸€æ­¥çš„å¹³æ»‘åŠ¨ä½œ
        self.state_master = None # [x1, x2, x3]
        self.state_slave = None  # [y1, y2, y3]
        # --- æ–°å¢ï¼šæ­¥æ•°è®¡æ•°å™¨ ---
        # self.current_step = 0
        # self.max_steps = 2000  # å¯¹åº” 2ç§’ çš„ä»¿çœŸ
        # é™åˆ¶æœ€å¤§æ­¥æ•°ï¼Œé˜²æ­¢å‘æ•£è¿‡ä¹…ï¼Œè¿™ç‚¹åœ¨æ³¨å†Œæ—¶å·²ç»è®¾ç½® max_episode_steps=2000ï¼Œ
        # æ‰€ä»¥å°±ä¸éœ€è¦åœ¨è¿™é‡Œé‡å¤è®¾ç½®äº†ï¼Œä¸‹é¢åŒç†
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # éšæœºåˆå§‹åŒ–ä¸»ä»ç³»ç»Ÿçš„çŠ¶æ€ï¼Œå¢åŠ è®­ç»ƒéš¾åº¦ï¼ˆæ³›åŒ–æ€§ï¼‰
        # self.current_step = 0 # é‡ç½®æ­¥æ•°
        # Master ç³»ç»Ÿåˆå§‹å€¼
        # æŒ‰ç…§è®ºæ–‡ï¼šåœ¨ [-10, 20] ä¹‹é—´éšæœºåˆå§‹åŒ–
        self.state_master = np.random.uniform(-10, 20, 3)
        # Slave ç³»ç»Ÿåˆå§‹å€¼ï¼ˆç»™å®ƒä¸€ä¸ªè¾ƒå¤§çš„åˆå§‹åå·®ï¼‰
        self.state_slave = np.random.uniform(-10, 20, 3)
         # ğŸŒŸ æ–°å¢ï¼šé‡ç½®ç¯å¢ƒæ—¶ï¼ŒåŠ¨ä½œæ»¤æ³¢å™¨çš„è®°å¿†ä¹Ÿè¦æ¸…é›¶
        self.filtered_action = np.zeros(2, dtype=np.float32)
        if self.add_noise:
            # æ¯æ¬¡é‡ç½®ç¯å¢ƒï¼Œå™ªå£°å¼ºåº¦éƒ½å˜ä¸€ä¸‹ï¼ŒèŒƒå›´ [0, 2]
            if self.eval_mode:
                # å¦‚æœæ˜¯è€ƒè¯•æ¨¡å¼ï¼Œé”å®šæœ€é«˜éš¾åº¦ 2.0ï¼Œä¿è¯å…¬å¹³å¯¹å¾…æ¯ä¸€ä¸ªæ¨¡å‹
                self.sigma = 2.0
            else:
                # å¦‚æœæ˜¯å¹³æ—¶è®­ç»ƒï¼ŒéšæœºæŠ½å¡ [0, 2]
                self.sigma = np.random.uniform(0, 2)
        else:
            self.sigma = 0.0
        # è¿”å›è®ºæ–‡è¦æ±‚çš„è§‚å¯Ÿå‘é‡ Ot (å…¬å¼ 12)
        error_vector = self.state_master - self.state_slave
        # è®°å¾— Reset ä¹Ÿè¦å½’ä¸€åŒ–
        normalized_error = np.clip(error_vector / self.scale_factor, -1.0, 1.0)
        # æ–°å¢ï¼šè·å– Master ç³»ç»Ÿçš„ç»å¯¹ä½ç½®å¹¶å½’ä¸€åŒ– (åˆå§‹å€¼æœ€å¤§ 20ï¼Œé™¤ä»¥ 20.0 å‹åˆ° [-1, 1] é™„è¿‘)
        normalized_master = np.clip(self.state_master / 20.0, -1.0, 1.0)
        # æ‹¼æ¥æˆ 6 ç»´å‘é‡ï¼š[ex, ey, ez, x_m, y_m, z_m]
        obs_6d = np.concatenate([normalized_error, normalized_master])
        return obs_6d.astype(np.float32), {}

    def step(self, action):
        # # self.current_step += 1
        # ğŸŒŸ æ ¸å¿ƒé˜²çº¿ 2ï¼šå‡ºå£é˜²çº¿ (åŠ¨ä½œæ»¤æ³¢)
        # æ‹¿åˆ°ç½‘ç»œç»™å‡ºçš„ç‹‚èº actionï¼Œåšå¹³æ»‘å¤„ç†ï¼
        # ==========================================
        if self.add_filter:
            action = (1 - self.action_alpha) * self.filtered_action + self.action_alpha * action
        # æ˜ å°„åŠ¨ä½œ
        # --- å…³é”®ä¿®æ”¹ï¼šæ‰‹åŠ¨å°† [-1, 1] æ˜ å°„åˆ° [-100, 100] ---
        # å‡è®¾ç½‘ç»œè¾“å‡ºçš„æ˜¯ raw_action
        a1 = np.clip(action[0], -1, 1) * 100.0
        a2 = np.clip(action[1], -1, 1) * 100.0

        # --- RK4 å‚æ•°å‡†å¤‡ ---
        dt = self.dt
        params = (self.a, self.b, self.c, self.d, self.r, self.s, self.I_bias, self.x_rest)

        # --- æ›´æ–° Master ç³»ç»Ÿ (æ³¨æ„ Master ä¸å— a1, a2 æ§åˆ¶) ---
        m_s = self.state_master
        mk1 = hr_derivatives(m_s, 0, 0, *params)
        mk2 = hr_derivatives(m_s + dt/2 * mk1, 0, 0, *params)
        mk3 = hr_derivatives(m_s + dt/2 * mk2, 0, 0, *params)
        mk4 = hr_derivatives(m_s + dt * mk3, 0, 0, *params)
        self.state_master += (dt/6.0) * (mk1 + 2*mk2 + 2*mk3 + mk4)

        # --- æ›´æ–° Slave ç³»ç»Ÿ (å— a1, a2 æ§åˆ¶) ---
        s_s = self.state_slave
        sk1 = hr_derivatives(s_s, a1, a2, *params)
        sk2 = hr_derivatives(s_s + dt/2 * sk1, a1, a2, *params)
        sk3 = hr_derivatives(s_s + dt/2 * sk2, a1, a2, *params)
        sk4 = hr_derivatives(s_s + dt * sk3, a1, a2, *params)
        self.state_slave += (dt/6.0) * (sk1 + 2*sk2 + 2*sk3 + sk4)

        
        # x1, x2, x3 = self.state_master
        # y1, y2, y3 = self.state_slave
        # # --- å…³é”®ä¿®æ”¹ï¼šæ‰‹åŠ¨å°† [-1, 1] æ˜ å°„åˆ° [-100, 100] ---
        # # å‡è®¾ç½‘ç»œè¾“å‡ºçš„æ˜¯ raw_action
        # a1 = np.clip(action[0], -1, 1) * 100.0 
        # a2 = np.clip(action[1], -1, 1) * 100.0

        # # --- è®ºæ–‡å…¬å¼ (10): Drive System (Master) ---
        # dx1 = x2 - self.a*(x1**3) + self.b*(x1**2) - x3 + self.I_bias
        # dx2 = self.c - self.d*(x1**2) - x2
        # dx3 = self.r * (self.s * (x1 - self.x_rest) - x3)
        
        # # --- è®ºæ–‡å…¬å¼ (13): Response System (Slave + Control) ---
        # dy1 = y2 - self.a*(y1**3) + self.b*(y1**2) - y3 + self.I_bias
        # dy2 = self.c - self.d*(y1**2) - y2 + a1  # æ§åˆ¶é‡ a1 åŠ åœ¨è¿™é‡Œ
        # dy3 = self.r * (self.s * (y1 - self.x_rest) - y3) + a2 # æ§åˆ¶é‡ a2 åŠ åœ¨è¿™é‡Œ
        # 3. äº§ç”Ÿ 3D é«˜æ–¯å™ªå£° (å‡è®¾æ ‡å‡†å·® sigma åœ¨ reset ä¸­å·²é‡‡æ ·)
        # ä»…åœ¨è®­ç»ƒæˆ–ç‰¹å®šæµ‹è¯•åœºæ™¯ä¸‹å¼€å¯
        # åªæœ‰åœ¨æµ‹è¯• Scenario B æˆ– C æ—¶ï¼Œæ‰æŠŠè¿™ä¸ªå¼€å…³æ‰“å¼€
        if self.add_noise:
            noise = np.random.normal(0, self.sigma, 3)
            self.state_master += noise * self.dt  # å™ªå£°éšæ­¥é•¿ç¼©æ”¾
            # # ä½œç”¨äº Master ç³»ç»Ÿ
            # dx1 += noise[0]
            # dx2 += noise[1]
            # dx3 += noise[2]
        

        # # æ›´æ–°æ•°å€¼ç§¯åˆ† (Euler æ–¹æ³•)
        # self.state_master += np.array([dx1, dx2, dx3]) * self.dt
        # self.state_slave += np.array([dy1, dy2, dy3]) * self.dt
        # --- åç»­é€»è¾‘ä¸å˜ (è®¡ç®— reward, observation ç­‰) ---
        # ä½¿ç”¨ RK4 æˆ–ç®€å•çš„æ¬§æ‹‰æ³•è¿›è¡Œä¸€é˜¶æ®µæ›´æ–° (ä¸ºäº†è®­ç»ƒé€Ÿåº¦ï¼Œè¿™é‡Œç¤ºä¾‹ç”¨æ”¹è¿›æ¬§æ‹‰)
        # --- è®ºæ–‡å…¬å¼ (12): è®¡ç®—æ–°çš„è§‚å¯Ÿå‘é‡ Ot ---
        error_vector = self.state_master - self.state_slave
        normalized_error = error_vector / self.scale_factor
        # æ–°å¢ï¼šåŒæ ·è·å– Master ç³»ç»Ÿçš„ç»å¯¹ä½ç½®å¹¶å½’ä¸€åŒ–
        normalized_master = np.clip(self.state_master / 20.0, -1.0, 1.0)
        
        # æ‹¼æ¥æˆ 6 ç»´å‘é‡
        obs_6d = np.concatenate([normalized_error, normalized_master])
        # 3. è®¡ç®—å¥–åŠ± (Reward Design - è®ºæ–‡çš„æ ¸å¿ƒ)
        # ç›®æ ‡ï¼šè¯¯å·®è¶Šå°å¥–åŠ±è¶Šé«˜ï¼Œæ§åˆ¶é‡è¶Šå°å¥–åŠ±è¶Šé«˜
        # --- è®ºæ–‡å…¬å¼ (14): å¥–åŠ±å‡½æ•° (æ›¼å“ˆé¡¿è·ç¦»çš„è´Ÿå€¼) ---
        # rt = -(|x1-y1| + |x2-y2| + |x3-y3|)
         # --- æ”¹åŠ¨ 3: å¥–åŠ±å‡½æ•°ä¿æŒä½¿ç”¨â€œçœŸå®è¯¯å·®â€ ---
        # ä¸ºä»€ä¹ˆè¦ç”¨çœŸå®è¯¯å·®ï¼Ÿå› ä¸ºæˆ‘ä»¬éœ€è¦ç‰©ç†æ„ä¹‰ä¸Šçš„æ”¶æ•›ã€‚
        # å¦‚æœç”¨å½’ä¸€åŒ–è¯¯å·®ï¼ŒReward æ•°å€¼å¤ªå°ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æƒé‡ã€‚
        # è¿™é‡Œçš„ 0.050 * action^2 æ˜¯æƒ©ç½š [-1,1] çš„åŠ¨ä½œè¾“å‡ºï¼Œæ˜¯åˆç†çš„
        reward = -np.sum(np.abs(normalized_error))- 0.050 * np.sum(np.square(action))
        
        # 4. åˆ¤æ–­ç»“æŸæ¡ä»¶
        # é€šå¸¸æ··æ²ŒåŒæ­¥è®­ç»ƒä¼šè·‘å›ºå®šçš„æ­¥æ•°ï¼Œæˆ–è€…è¯¯å·®è¿‡å¤§æ—¶å¼ºåˆ¶åœæ­¢
        terminated = False
        truncated = False
        # --- æ”¹åŠ¨ 4: å¢åŠ  Early Stopping (é˜²å‘æ•£) ---
        # å¦‚æœä»»ä½•ä¸€ä¸ªç»´åº¦çš„è¯¯å·®è¶…è¿‡ 70 (æ¯”50å¤§ä¸€ç‚¹)ï¼Œè®¤ä¸ºæ§åˆ¶å¤±è´¥ï¼Œå¼ºåˆ¶ç»“æŸ
        # è¿™èƒ½æå¤§åœ°åŠ é€Ÿè®­ç»ƒï¼Œä¸è®© Agent åœ¨é”™è¯¯çš„é“è·¯ä¸Šæµªè´¹æ—¶é—´
        if np.any(np.abs(error_vector) > 70.0):
            terminated = True
            reward = -2000.0  # ç»™ä¸€ä¸ªå¤§çš„æƒ©ç½š
        # truncated = self.current_step >= self.max_steps  # 2000æ­¥åå¼ºåˆ¶ç»“æŸå¹¶é‡ç½®
        
        return obs_6d.astype(np.float32), float(reward), terminated, truncated, {}

# if __name__ == "__main__":
#     env = HRSyncEnv(add_noise=False)  # å¯ç”¨å™ªå£°
#     model = PPO("MlpPolicy", env, learning_rate=3e-4,      # é™ä½å­¦ä¹ ç‡ï¼Œèµ°ç¨³ä¸€ç‚¹
#     n_steps=2048,            # æ¯æ¬¡æ›´æ–°é‡‡é›†æ›´å¤šæ ·æœ¬
#     batch_size=64,           # å‡å° Batch å¢åŠ æ›´æ–°é¢‘ç‡
#     gae_lambda=0.95,         # ç¨³å®šä¼˜åŠ¿ä¼°è®¡
#     verbose=1,
#     device='cpu')
#     print("æ­£åœ¨è®­ç»ƒè®ºæ–‡ç‰ˆæ¨¡å‹ï¼Œè¯·ç¨å€™...")
#         #  100 ä¸‡æ­¥
#     model.learn(total_timesteps=1000000)
#     model.save("hr_paper_final_model") # è¿™ä¼šç”Ÿæˆä¸€ä¸ª .zip æ–‡ä»¶
#     print("è®­ç»ƒæˆåŠŸï¼æ¨¡å‹å·²ä¿å­˜ä¸º hr_paper_final_model.zip")